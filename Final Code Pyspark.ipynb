{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Preprocessing\n",
    "data = pd.read_csv('job_dataset_test.csv',encoding ='latin-1')\n",
    "data=data.dropna(how='all')\n",
    "cols = [\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\"]\n",
    "for c in cols:\n",
    "    data[c] = data[c].fillna(\"\")    \n",
    "def extract_features(data):    \n",
    "    for c in cols:\n",
    "        data[c+\"_len\"] = data[c].apply(lambda x : len(str(x)))\n",
    "        data[c+\"_wc\"] = data[c].apply(lambda x : len(str(x.split())))    \n",
    "extract_features(data)\n",
    "\n",
    "data['combined_text'] = data['company_profile'] + \" \" + data['description'] + \" \" + data['requirements'] + \" \" + data['benefits']\n",
    "n_features = {\n",
    "    \"title\" : 100,\n",
    "    \"combined_text\" : 500\n",
    "}\n",
    "for c, n in n_features.items():\n",
    "    tfidf = TfidfVectorizer(max_features=n, stop_words = 'english')\n",
    "    tfidf.fit(data[c])\n",
    "    tfidf_train = np.array(tfidf.transform(data[c]).toarray(), dtype=np.float16)\n",
    "    \n",
    "    for i in range(n_features[c]):\n",
    "        \n",
    "        data[c+\"_tfidf_\"+ str(i)]= tfidf_train[:, i]        \n",
    "drop_cols = ['title', 'location', 'department', 'salary_range', 'company_profile', \n",
    "             'description', 'requirements', 'benefits', 'combined_text']\n",
    "data=data.drop(drop_cols,axis=1)  \n",
    "\n",
    "# One Hot Encoding\n",
    "data=pd.get_dummies(data)\n",
    "data.shape\n",
    "data_train= data.drop(data.tail(5).index)\n",
    "data_input= data[-5:]\n",
    "data_test=data_input.drop(\"fraudulent\",axis=1)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n",
    "df=spark.createDataFrame(data_train)\n",
    "df_test=spark.createDataFrame(data_test)\n",
    "\n",
    "# Up Sampling \n",
    "major_df = df.filter(col(\"fraudulent\") == 0)\n",
    "minor_df = df.filter(col(\"fraudulent\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "a = range(ratio)\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "df_upsampled = major_df.unionAll(oversampled_df)\n",
    "input_features=[]\n",
    "for item in df_upsampled.dtypes:\n",
    "    input_features.append(item[0])    \n",
    "input_features.remove(\"fraudulent\")\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(inputCols= input_features, outputCol=\"features\")\n",
    "input_df= assembler.transform(df_upsampled).select(\"fraudulent\", \"features\")\n",
    "input_test= assembler.transform(df_test).select(\"features\")\n",
    "\n",
    "# Data Modelling\n",
    "gbt = GBTClassifier(labelCol=\"fraudulent\", featuresCol=\"features\", maxIter=50)\n",
    "rf=RandomForestClassifier(labelCol=\"fraudulent\", featuresCol=\"features\",numTrees=50)\n",
    "lsvc = LinearSVC(labelCol=\"fraudulent\", featuresCol=\"features\",maxIter=50, regParam=0.1)\n",
    "inpt=[0.25, 0.50, 0.75, 1.0]\n",
    "models=[gbt, rf, lsvc]\n",
    "acc_lst=[]\n",
    "precis_lst=[]\n",
    "rec_lst=[]\n",
    "area_uc_lst=[]\n",
    "time_lst=[]\n",
    "for i in inpt:    \n",
    "    input_df_new = input_df.sample(False,i, 42)    \n",
    "    train_df, test_df = input_df_new.randomSplit([.8,.2])\n",
    "    print(\"{}% of data has records = {}\".format(i, input_df_new.count()))    \n",
    "    acc=[]\n",
    "    precis=[]\n",
    "    rec=[]\n",
    "    area_uc=[]\n",
    "    time_taken=[]    \n",
    "    for item in models:        \n",
    "        start = time.time()\n",
    "        model = item.fit(train_df)\n",
    "        print(item)\n",
    "        predictions = model.transform(test_df)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"fraudulent\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        print(\"Accuracy:\",accuracy)\n",
    "        acc.append(accuracy)        \n",
    "        evaluator1 = MulticlassClassificationEvaluator(labelCol=\"fraudulent\", predictionCol=\"prediction\", \n",
    "                                              metricName=\"precisionByLabel\", metricLabel= 1.0)\n",
    "        precision = evaluator1.evaluate(predictions)\n",
    "        print(\"Precision:\",precision)\n",
    "        precis.append(precision)        \n",
    "        evaluator2 = MulticlassClassificationEvaluator(labelCol=\"fraudulent\", predictionCol=\"prediction\", \n",
    "                                                      metricName=\"recallByLabel\", metricLabel= 1.0)\n",
    "        recall = evaluator2.evaluate(predictions)\n",
    "        print(\"Recall:\",recall)\n",
    "        rec.append(recall)        \n",
    "        evaluator3 = BinaryClassificationEvaluator(labelCol=\"fraudulent\", metricName='areaUnderROC')\n",
    "        auc = evaluator3.evaluate(predictions)\n",
    "        print(\"AUC:\",auc)\n",
    "        area_uc.append(auc)        \n",
    "        end = time.time()\n",
    "        time_diff= end - start\n",
    "        print(time_diff)\n",
    "        time_taken.append(time_diff)        \n",
    "        print()        \n",
    "    acc_lst.append(acc) \n",
    "    precis_lst.append(precis)\n",
    "    rec_lst.append(rec)\n",
    "    area_uc_lst.append(area_uc)\n",
    "    time_lst.append(time_taken)    \n",
    "    print(\"------------------------------------\")    \n",
    "print(\"List of Accuracy:{}\".format(acc_lst))\n",
    "print(\"List of Precision:{}\".format(precis_lst))\n",
    "print(\"List of Recall:{}\".format(rec_lst))\n",
    "print(\"List of AUC:{}\".format(area_uc_lst))\n",
    "print(\"List of time taken:{}\".format(time_lst))\n",
    "\n",
    "# Data Visualization\n",
    "acc_y1=[]\n",
    "acc_y2=[]\n",
    "acc_y3=[]\n",
    "for item in acc_lst:\n",
    "    acc_y1.append(round(item[0],3))\n",
    "    acc_y2.append(round(item[1],3))\n",
    "    acc_y3.append(round(item[2],3))    \n",
    "acc_label=[\"25%\",\"50%\",\"75%\",\"100%\"]\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(10, 5), dpi=200)\n",
    "ax.plot(acc_label, acc_y1,marker=\"o\",\n",
    "        markerfacecolor='white', markeredgewidth=2, linewidth=2.5, label=\"GBT\")\n",
    "ax.plot(acc_label, acc_y2,marker=\"o\", markerfacecolor='white', \n",
    "        markeredgewidth=2, linewidth=2.5, label=\"RF\")\n",
    "ax.plot(acc_label, acc_y3,marker=\"o\", markerfacecolor='white', \n",
    "        markeredgewidth=2, linewidth=2.5, label=\"SVM\")\n",
    "acc_all=[acc_y1,acc_y2,acc_y3]\n",
    "for item in acc_all:        \n",
    "    for i,j in zip(acc_label,item):        \n",
    "        ax.text(i, j-0.005, str(j))\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.0, 1.10),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "ax.set_ylim([0.875,0.975])\n",
    "ax.set_xlabel(\"Scaling\",  fontsize=14)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "ax.set_title(\"Accuracy vs Scaling\",loc=\"left\", fontsize=18)\n",
    "fig.savefig(\"accuracy.png\",dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
